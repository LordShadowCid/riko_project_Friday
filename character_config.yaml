# Ollama configuration (local LLM)
ollama:
  host: http://localhost:11434
  # Uncensored models (recommended for roleplay):
  #   mannix/llama3.1-8b-abliterated - Llama 3.1 with censorship removed (abliterated)
  #   dolphin3:8b - Dolphin 3.0, designed to be uncensored/steerable
  # Standard models:
  #   qwen3:14b - Smartest but slowest
  #   llama3.1:8b - Fast but censored
  model: mannix/llama3.1-8b-abliterated
  # Keep model loaded in memory for faster responses (in seconds, -1 = forever)
  keep_alive: 3600
  # Enable streaming for faster first response
  stream: true
  # Reduce context for faster generation (2048 is enough for chat)
  num_ctx: 2048

# RAM-based response cache for faster repeated queries
# Caches recent query-response pairs to avoid regenerating similar responses
response_cache:
  enabled: true
  # Maximum number of responses to cache (uses ~1KB per entry)
  max_entries: 100

history_file: chat_history.json
model: "mannix/llama3.1-8b-abliterated"
# Reduced from 15 to 8 for faster LLM response (smaller context window)
max_history_turns: 8

# Windows/local runtime settings
audio:
  # Set to an integer device index OR a substring of the device name.
  # Leave blank/null to use the system default.
  input_device: BCC950
  output_device: BCC950

whisper:
  # Common choices: base.en, small.en, medium.en, large-v3, turbo
  model: base.en
  # cpu or cuda
  device: cuda
  # Which GPU to use (0=RTX A4000, 1=Quadro RTX 4000)
  # GPU 1 is dedicated for Whisper while GPU 0 runs the LLM
  device_index: 1
  # float32 (cpu), float16/int8_float16 (cuda), int8 (cpu/cuda)
  compute_type: float16
  # Fallback to CPU if CUDA fails
  fallback_to_cpu: true

# Voice Activity Detection settings (hands-free mode)
vad:
  # Enable hands-free mode (true) or push-to-talk (false)
  enabled: true
  # VAD sensitivity for detecting when you START speaking: 0-3
  # 0 = most sensitive (picks up quiet speech), 3 = least sensitive (requires louder/clearer speech)
  # If you're getting false triggers from background noise, increase this to 3
  aggressiveness: 3
  # How long to wait after you stop speaking before processing (seconds)
  silence_threshold_sec: 1.0
  
  # ========== Interrupt Detection Settings ==========
  # These control how Annabeth detects when you want to interrupt her
  # Higher values = less sensitive (fewer false interrupts from background noise)
  
  # 3 = maximum filtering, only clear speech triggers interrupt
  interrupt_aggressiveness: 3
  
  # Number of consecutive speech frames needed to trigger interrupt
  # At 30ms per frame: 15 frames = ~450ms of sustained speech required
  # Increase this if you're still getting false interrupts (try 20-25)
  interrupt_speech_frames: 15
  
  # Minimum audio volume (RMS energy) to even consider as potential speech
  # This filters out quiet background noise before VAD even checks it
  # Range: 0-32768 (higher = requires louder speech to interrupt)
  # 500 = moderate, 1000 = requires clearly audible speech
  # Increase this if background noise keeps triggering interrupts
  interrupt_min_energy: 800

# Speaker Identification settings
# Allows Annabeth to know WHO is speaking (e.g., you vs your daughter)
speaker_id:
  # Enable speaker identification
  enabled: true
  # Minimum similarity score (0-1) to consider a match
  # Lower = more lenient, Higher = stricter matching
  # 0.70 = lenient (may confuse similar voices)
  # 0.80 = strict (may not recognize if voice differs)
  threshold: 0.75
  # Log speaker identification results
  debug: true

presets:
  default:
    system_prompt: |
      You are Annabeth, a helpful voice assistant with a snarky anime girl personality.
      
      SPEAKER IDENTIFICATION:
      - Messages may include a speaker tag like "[Dad]: hello" or "[Riley]: hi there"
      - The tag tells you WHO is speaking, NOT what to call them
      - [Dad] is your primary user - address him as "senpai", "master", or "boss" (vary these affectionately)
      - [Riley] is his daughter - you can call her by name or use cute nicknames
      - If there's no tag or [Unknown], assume it's your primary user (senpai)
      - NEVER call your primary user "Dad" - that's just his speaker ID tag
      
      IMPORTANT RULES FOR VOICE OUTPUT:
      - Speak naturally as if having a real conversation
      - NEVER use asterisks or action markers like *laughs* or *sighs*
      - NEVER spell out words letter-by-letter
      - NEVER use ALL CAPS for emphasis
      - Keep responses conversational and concise (1-3 sentences usually)
      - Your text will be read aloud by a text-to-speech system, so write exactly what should be spoken

sovits_ping_config:
  text_lang: en
  prompt_lang : en
  # For Docker GPT-SoVITS, this should be a Linux path inside the container.
  # The provided docker-compose mounts ./character_files to /data/ref
  ref_audio_path : /data/ref/main_sample.wav
  prompt_text : This is a sample voice for you to just get started with because it sounds kind of cute but just make sure this doesn't have long silences.
  